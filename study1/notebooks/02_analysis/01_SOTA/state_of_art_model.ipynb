{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import re\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ryo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ryo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ryo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "# Assume your dataset is loaded into a pandas dataframe named `df`\n",
    "# The dataframe should have two columns: 'text' and 'type'\n",
    "# df = pd.read_csv('your_dataset.csv') \n",
    "\n",
    "file_path = \"C:/Users/Ryo/OneDrive/Desktop/Master Thesis/study/study1/raw/sample_forFeeding.xlsx\"\n",
    "df = pd.read_excel(file_path, sheet_name='with_label')\n",
    "df = df.rename(columns = {\"type\" : \"type\",\n",
    "                          \"posts_filtered\" : \"text\"})\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess text\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove links\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13216510, 14624000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['type'] = label_encoder.fit_transform(df['type'])\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['type'], test_size=0.3, random_state=42, stratify=df['type'])\n",
    "\n",
    "# Vectorize text using Word2Vec\n",
    "tokenized_train = [text.split() for text in X_train]\n",
    "tokenized_test = [text.split() for text in X_test]\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=tokenized_train, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_model.train(tokenized_train, total_examples=len(tokenized_train), epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecEstimator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vector_size=100, window=5, min_count=1, workers=4, epochs=50):\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.min_count = min_count\n",
    "        self.workers = workers\n",
    "        self.epochs = epochs\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.model = Word2Vec(sentences=X, vector_size=self.vector_size, window=self.window, \n",
    "                              min_count=self.min_count, workers=self.workers)\n",
    "        self.model.train(X, total_examples=len(X), epochs=self.epochs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([self.get_average_word2vec(s) for s in X])\n",
    "\n",
    "    def get_average_word2vec(self, tokens_list):\n",
    "        if len(tokens_list) < 1:\n",
    "            return np.zeros(self.vector_size)\n",
    "        vec = []\n",
    "        for token in tokens_list:\n",
    "            try:\n",
    "                vec.append(self.model.wv[token])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return np.mean(vec, axis=0) if vec else np.zeros(self.vector_size)\n",
    "\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'vector_size': [50, 100, 200],\n",
    "    'window': [3, 5, 7],\n",
    "    'min_count': [1, 2, 3],\n",
    "    'epochs': [30, 50, 70]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryo\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'epochs': 30, 'min_count': 1, 'vector_size': 50, 'window': 3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7928675, 8774400)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dummy target variable (not used, but required by GridSearchCV)\n",
    "dummy_y = np.zeros(len(tokenized_train))\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(Word2VecEstimator(), param_grid, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(tokenized_train, dummy_y)  # Use dummy_y instead of tokenized_train\n",
    "\n",
    "# Get best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best parameters:\", best_params)\n",
    "\n",
    "# Train Word2Vec model with best parameters\n",
    "word2vec_model = Word2Vec(sentences=tokenized_train, vector_size=best_params['vector_size'], \n",
    "                          window=best_params['window'], min_count=best_params['min_count'], workers=4)\n",
    "word2vec_model.train(tokenized_train, total_examples=len(tokenized_train), epochs=best_params['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get average word vectors\n",
    "def get_average_word2vec(tokens_list, vector, k=best_params['vector_size']):\n",
    "    if len(tokens_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    vec = []\n",
    "    for token in tokens_list:\n",
    "        try:\n",
    "            vec.append(vector[token])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return np.mean(vec, axis=0) if vec else np.zeros(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word vectors for train and test sets\n",
    "X_train_word2vec = [get_average_word2vec(s, word2vec_model.wv) for s in tokenized_train]\n",
    "X_test_word2vec = [get_average_word2vec(s, word2vec_model.wv) for s in tokenized_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_word2vec = [get_average_word2vec(s, word2vec_model.wv) for s in tokenized_train]\n",
    "#X_test_word2vec = [get_average_word2vec(s, word2vec_model.wv) for s in tokenized_test]\n",
    "\n",
    "# Handle imbalanced data with SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_word2vec, y_train)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'SVM': SVC(kernel='linear'),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    'CatBoost': CatBoostClassifier(verbose=0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: F1 Score = 0.1030\n",
      "SVM: F1 Score = 0.0967\n",
      "Random Forest: F1 Score = 0.0755\n",
      "XGBoost: F1 Score = 0.1132\n",
      "CatBoost: F1 Score = 0.0757\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    y_pred = model.predict(X_test_word2vec)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    results[model_name] = f1\n",
    "\n",
    "# Print results\n",
    "for model_name, f1 in results.items():\n",
    "    print(f\"{model_name}: F1 Score = {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
